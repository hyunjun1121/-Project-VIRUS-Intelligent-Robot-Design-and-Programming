[
  {
    "type": "arxiv_id",
    "arxiv_id": "2301.12345",
    "description": "arXiv ID를 사용한 자동 논문 다운로드 및 처리 예시"
  },
  {
    "type": "text",
    "title": "Attention Is All You Need",
    "full_text": "Abstract\n\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely...\n\n1 Introduction\n\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation...",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"],
    "published": "2017-06-12",
    "url": "https://arxiv.org/abs/1706.03762",
    "description": "직접 텍스트 입력을 사용한 처리 예시"
  },
  {
    "type": "text",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "full_text": "Abstract\n\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers...\n\n1 Introduction\n\nLanguage model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering...",
    "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"],
    "published": "2018-10-11",
    "url": "https://arxiv.org/abs/1810.04805",
    "description": "BERT 논문 처리 예시"
  }
]