from multiprocessing import process
import os
import io
import queue
import time
import json
from typing import Any
import numpy as np
import sounddevice as sd
import soundfile as sf
from dotenv import load_dotenv
import openai
import threading
import pygame
# Import the modules needed for the complete system
from LLM_function import process_voice_text as process_for_commands, process_voice_audio as process_for_audio_commands
from LLM_conversation import process_voice_text as process_for_conversation, process_voice_audio as process_for_audio_conversation
from text_to_audio import text_to_speech
from client_vlm_parallel_alt import main as run_vlm_alt

try:
    from pi_exercise import main as run_spike
    SPIKE_AVAILABLE = True
    print("âœ… SPIKE robot control available")
except ImportError as e:
    print(f"âš ï¸ SPIKE robot control unavailable: {e}")
    SPIKE_AVAILABLE = False
    def run_spike(command):
        print(f"ğŸ¤– [SIMULATION] Would execute robot command: {command}")

# Load environment variables for API keys
load_dotenv()
api_lock = threading.Lock()
sound_lock = threading.Lock()

def convert_audio_to_text_via_api(audio_data, sample_rate):
    """Converts audio data to text using OpenAI's Whisper API."""
    global client # ì „ì—­ OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©

    if audio_data is None or len(audio_data) == 0:
        print("âŒ No audio data to transcribe.")
        return None

    try:
        wav_buffer = io.BytesIO()
        # Whisper APIëŠ” ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í˜•ì‹ì„ ì§€ì›í•˜ì§€ë§Œ, WAVê°€ ì¼ë°˜ì ì…ë‹ˆë‹¤.
        # LLM_function.py/LLM_conversation.py ì—ì„œ ì‚¬ìš©ëœ PCM_U8ì„ ìœ ì§€í•˜ì—¬ íŒŒì¼ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.
        sf.write(wav_buffer, audio_data, sample_rate, format="WAV", subtype="PCM_U8")
        wav_buffer.name = "audio_for_stt.wav" # íŒŒì¼ ì´ë¦„ ëª…ì‹œ (API ì¼ë¶€ì—ì„œ í•„ìš”í•  ìˆ˜ ìˆìŒ)
        wav_buffer.seek(0)

        print(f"ğŸ“¤ Uploading audio ({len(wav_buffer.getvalue())/1024:.1f}KB) to Whisper API...")
        
        # client.audio.transcriptions.createëŠ” íŒŒì¼ ê°ì²´ë¥¼ ì§ì ‘ ë°›ìŠµë‹ˆë‹¤.
        response = client.audio.transcriptions.create(
            model="gpt-4o-mini-transcribe",
            file=wav_buffer, # wav_bufferëŠ” BytesIO ê°ì²´ì´ë¯€ë¡œ íŒŒì¼ì²˜ëŸ¼ ë™ì‘í•©ë‹ˆë‹¤.
            response_format="text",
            language="en" # ì˜ì–´ë¡œ ê³ ì • (í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
        )
        
        # Whisper API ì‘ë‹µì€ í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìì²´ì…ë‹ˆë‹¤ (response_format="text"ì¸ ê²½ìš°).
        transcribed_text = response.strip() if response else None
        return transcribed_text
    except Exception as e:
        print(f"âŒ Whisper API transcription error: {e}")
        return None

class MultiThreadManager:
    def __init__(self):
        self.vlm_thread = None
        self.vlm_result = None
        self.vlm_complete = threading.Event()
        
        self.conversation_thread = None
        self.conversation_result = None
        self.conversation_complete = threading.Event()
        
        # self.face_thread = None
        # self.face_result = None
        # self.face_complete = threading.Event()
        
        self.lock = threading.Lock()
    def start_vlm_processing(self):
        with self.lock:
            if self.vlm_thread is None or not self.vlm_thread.is_alive():
                self.vlm_complete.clear()
                self.vlm_thread = threading.Thread(
                    target=self._run_vlm,
                    daemon=True
                )
                self.vlm_thread.start()
            else:
                print("[VLM] Processing already running")
    def _run_vlm(self):
        try:
            result = run_vlm_alt(mode="image")
            with self.lock:
                self.vlm_result = result
        except Exception as e:
            print(f"VLM error: {e}")
            with self.lock:
                self.vlm_result = None
        finally:
            self.vlm_complete.set()
    def get_vlm_result(self, timeout=30):
        finished = self.vlm_complete.wait(timeout)
        with self.lock:
            return self.vlm_result if finished else None
    def start_text_conversation_processing(self, transcribed_text, vlm_result):
        with self.lock:
            if self.conversation_thread is None or not self.conversation_thread.is_alive():
                self.conversation_complete.clear()
                self.conversation_thread = threading.Thread(
                    target=self._run_text_conversation,
                    args = [transcribed_text, vlm_result],
                    daemon=True
                )
                self.conversation_thread.start()
            else:
                print("[Conversation] Processing already running")
    def _run_text_conversation(self, transcribed_text, vlm_result):
        result = False
        try:
            with api_lock:
                conversation_response_text = process_for_conversation(
                    transcribed_text,
                    additional_prompt=f"[Image/Video Description]: {vlm_result}"
                )
            
            if conversation_response_text:
                print("\nğŸ”Š Converting response to speech...")
                response_file_path = text_to_speech(
                    text=conversation_response_text,
                    voice_id=VOICE_ID,
                    output_filename=RESPONSE_AUDIO_FILE
                )
                result = True
                
                if response_file_path and os.path.exists(response_file_path):
                    print(f"Conversational audio response saved to {RESPONSE_AUDIO_FILE}")
                    # Try to play the audio response
                    try:
                        import pygame
                        pygame.mixer.init()
                        pygame.mixer.music.load(response_file_path)
                        pygame.mixer.music.play()
                        print("ğŸ”Š Playing response audio...")
                        while pygame.mixer.music.get_busy():
                            pygame.time.Clock().tick(10)
                    except ImportError:
                        print(f"Audio saved to {RESPONSE_AUDIO_FILE}. Install pygame to enable autoplay.")
                    except Exception as e:
                        print(f"Error playing audio: {e}")
            else:
                print("âš ï¸ No conversational response generated.")
                
            with self.lock:
                self.conversation_result = result
        except Exception as e:
            print(f"Conversation (text input) error: {e}")
            with self.lock:
                self.conversation_result = False
        finally:
            self.conversation_complete.set()
    def get_conversation_result(self, timeout=30):
        finished = self.conversation_complete.wait(timeout)
        with self.lock:
            return self.conversation_result if finished else None
    # def start_converation_processing(self, audio_data, vlm_result):
    #     def _converation_wrapper():
    #         self.conversation_running.set()
    #         try:
    #             with api_lock:
    #                 conversation_response = process_for_audio_conversation(
    #                 audio_data,
    #                 SAMPLE_RATE,
    #                 additional_prompt=f"[Image/Video Description]: {vlm_result}"
    #                 )
    #             if conversation_response:
    #                 print("\nğŸ”Š Converting response to speech...")
    #                 text_to_speech(
    #                     text=conversation_response,
    #                     voice_id=VOICE_ID,
    #                     output_filename=OUTPUT_AUDIO_FILE
    #                 )
    #                 # Optionally play the audio response
    #                 try:
    #                     print("pygame start")
    #                     # pygame.mixer.init()
    #                     # pygame.mixer.music.load(OUTPUT_AUDIO_FILE)
    #                     # pygame.mixer.music.play()
    #                     print("pypgame end")
    #                     # while pygame.mixer.music.get_busy():
    #                     #     pygame.time.Clock().tick(10)
    #                 except ImportError:
    #                     print(f"Audio saved to {OUTPUT_AUDIO_FILE}. Install pygame to enable autoplay.")
    #         finally:
    #             print("conversation end")
    #             self.conversation_running.clear()
    #     self.conversation_thread = threading.Thread(target=_converation_wrapper, daemon=True)
    #     self.conversation_thread.start()
    # def start_face_processing(self):
    #     with self.lock:
    #         if self.face_thread is None or not self.face_thread.is_alive():
    #             self.face_complete.clear()
    #             self.face_thread = threading.Thread(
    #                 target=self._run_face,
    #                 daemon=True
    #             )
    #             self.face_thread.start()
    #         else:
    #             print("[face] Processing already running")
    # def _run_face(self):
    #     try:
    #         with self.lock:
    #             self.face_result = run_face_alt(mode="image")
    #     except Exception as e:
    #         print(f"face error: {e}")
    #         with self.lock:
    #             self.face_result = None
    #     finally:
    #         self.face_complete.set()
    # def get_face_result(self, timeout=30):
    #     finished = self.face_complete.wait(timeout)
    #     with self.lock:
    #         return self.face_result if finished else None




manager = MultiThreadManager()
# Check if OpenAI API key is available
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OpenAI API key is not set. Please check your .env file.")

# Initialize OpenAI client
client = openai.OpenAI()

# Audio recording settings - Optimized for Raspberry Pi
SAMPLE_RATE = 8000     # Reduced from 16000 for faster upload (still decent quality)
CHANNELS = 1           # Mono
VOICE_ID = "ErXwobaYiN019PkySvjV"    # Voice ID for ElevenLabs - antoni (ë‚¨ì„±, ë¯¸êµ­ ì–µì–‘)
WAIT_AUDIO_FILE = "wait.mp3"  # Fixed wait message file
RESPONSE_AUDIO_FILE = "response.mp3"  # Response audio file (generated by text_to_audio.py)
OUTPUT_AUDIO_FILE = "out.mp3"
# ì†Œë¦¬ ê°ì§€ ì„¤ì • - Raspberry Pi optimized
THRESHOLD_DB = -35     # ë…¹ìŒ ì‹œì‘ì„ ìœ„í•œ ì„ê³„ ë°ì‹œë²¨ (ì ì ˆí•œ ê°’ìœ¼ë¡œ ì¡°ì • í•„ìš”)
SILENCE_THRESHOLD_DB = -35  # ë…¹ìŒ ì¢…ë£Œë¥¼ ìœ„í•œ ì„ê³„ ë°ì‹œë²¨ (ì ì ˆí•œ ê°’ìœ¼ë¡œ ì¡°ì • í•„ìš”)
SILENCE_DURATION = 1.0  # ë…¹ìŒ ì¢…ë£Œë¥¼ ìœ„í•œ ì¹¨ë¬µ ì§€ì† ì‹œê°„(ì´ˆ) - ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ 1ì´ˆë¡œ ë‹¨ì¶•
# MAX_RECORDING_DURATION = None  # ìµœëŒ€ ë…¹ìŒ ì‹œê°„ ì œí•œ ì—†ìŒ

frames = []
recording = False
recording_start_time = None  # ë…¹ìŒ ì‹œì‘ ì‹œê°„ ì¶”ì  - ì¶”ê°€
silence_start_time = None
last_db_print_time = 0  # ë°ì‹œë²¨ ì¶œë ¥ ì œí•œì„ ìœ„í•œ ë§ˆì§€ë§‰ ì¶œë ¥ ì‹œê°„
last_countdown_time = 0  # ì¹´ìš´íŠ¸ë‹¤ìš´ ì¶œë ¥ ì œí•œì„ ìœ„í•œ ë§ˆì§€ë§‰ ì¶œë ¥ ì‹œê°„
DB_PRINT_INTERVAL = 0.5  # ë°ì‹œë²¨ ì¶œë ¥ ê°„ê²©(ì´ˆ)
COUNTDOWN_PRINT_INTERVAL = 0.2  # ì¹´ìš´íŠ¸ë‹¤ìš´ ì¶œë ¥ ê°„ê²©(ì´ˆ)
recording_completed = False  # ë…¹ìŒ ì™„ë£Œ í”Œë˜ê·¸ (ë…¹ìŒë§Œ ì¤‘ë‹¨í•˜ê³  ì´í›„ ì²˜ë¦¬ëŠ” ê³„ì†í•¨)
processing_audio = False
def calculate_db(audio_data):
    """ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ ë°ì‹œë²¨ ë ˆë²¨ ê³„ì‚°"""
    if len(audio_data) == 0:
        return -np.inf
    # RMS ê°’ ê³„ì‚°
    rms = np.sqrt(np.mean(np.square(audio_data)))
    # RMSë¥¼ dBë¡œ ë³€í™˜ (0 dB ê¸°ì¤€ì€ ìµœëŒ€ ê°€ëŠ¥ ì§„í­ 1.0)
    if rms > 0:
        db = 20 * np.log10(rms)
    else:
        db = -np.inf
    return db
record_count = 0
def audio_callback(indata, frames_count, time_info, status):
    """Callback function for audio stream"""
    global frames, recording, recording_start_time, silence_start_time, last_db_print_time, last_countdown_time, recording_completed, record_count,manager, api_lock, processing_audio
    if status:
        print(f"âš ï¸ Recording warning: {status}")
    
    # ì´ë¯¸ ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìœ¼ë©´ ë°ì´í„° ìˆ˜ì§‘í•˜ì§€ ì•ŠìŒ
    if recording_completed:
        return
    
    # í˜„ì¬ ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ ë°ì‹œë²¨ ë ˆë²¨ ê³„ì‚°
    current_db = calculate_db(indata)
    
    # í˜„ì¬ ì‹œê°„
    current_time = time.time()
    
    # ì¼ì • ê°„ê²©ìœ¼ë¡œ í˜„ì¬ ë°ì‹œë²¨ ì¶œë ¥
    if current_time - last_db_print_time >= DB_PRINT_INTERVAL:
        db_status = ""
        if current_db > THRESHOLD_DB:
            db_status = "ğŸ”Š ACTIVE"
        elif current_db < SILENCE_THRESHOLD_DB:
            db_status = "ğŸ”ˆ SILENT"
        else:
            db_status = "ğŸ”‰ NORMAL"
        
        print(f"ğŸ¤ Sound level: {current_db:.1f} dB {db_status}")
        last_db_print_time = current_time
    
    # ë…¹ìŒ ì¤‘ì´ ì•„ë‹ ë•Œ, ì„ê³„ê°’ ì´ìƒì´ë©´ ë…¹ìŒ ì‹œì‘
    if not recording and current_db > THRESHOLD_DB:
        recording = True
        frames.clear()
        silence_start_time = None
        recording_start_time = current_time
        record_count=0
        print(f"\nâºï¸ Recording started automatically (detected {current_db:.2f} dB > threshold {THRESHOLD_DB} dB)...")

        # Trigger image or video upload via client_vlm_parallel_alt
        

        # Start a thread pool executor to upload image or video and get result
        # executor = concurrent.futures.ThreadPoolExecutor()
        # future_vlm = executor.submit(run_vlm_alt, mode="image")  # or "video"
        # vlm_thread = threading.Thread(
        #     target = lambda: vlm_result_queue.put(run_vlm_alt(mode="image")),
        #     daemon=True
        # )
        # vlm_thread.start()
        manager.start_vlm_processing()
    # ë…¹ìŒ ì¤‘ì¼ ë•Œ
    if recording:
        frames.append(indata.copy())
        record_count+=1
        # ìµœëŒ€ ë…¹ìŒ ì‹œê°„ ì œí•œ ì—†ìŒìœ¼ë¡œ ë³€ê²½ - ì£¼ì„ ì²˜ë¦¬
        # recording_duration = current_time - recording_start_time
        # if recording_duration >= MAX_RECORDING_DURATION:
        #     recording = False
        #     recording_completed = True
        #     print(f"\nâ¹ï¸ Recording ended automatically (max duration {MAX_RECORDING_DURATION} seconds reached).")
        #     process_recorded_audio()
        #     return
        
        # ì„ê³„ê°’ ì´í•˜ì˜ ì†Œë¦¬ê°€ ê°ì§€ë˜ë©´ ì¹¨ë¬µ ì‹œê°„ ì²´í¬ ì‹œì‘/ì—…ë°ì´íŠ¸
        if current_db < SILENCE_THRESHOLD_DB:
            if silence_start_time is None:
                silence_start_time = current_time
                print(f"\nâ¸ï¸ Silence detected ({current_db:.2f} dB < threshold {SILENCE_THRESHOLD_DB} dB)")
            
            # ì¹¨ë¬µ ê²½ê³¼ ì‹œê°„
            elapsed_silence = current_time - silence_start_time
            # ì¢…ë£Œê¹Œì§€ ë‚¨ì€ ì‹œê°„
            remaining_time = SILENCE_DURATION - elapsed_silence
            
            # ì¼ì • ê°„ê²©ìœ¼ë¡œ ì¹´ìš´íŠ¸ë‹¤ìš´ ì¶œë ¥
            if current_time - last_countdown_time >= COUNTDOWN_PRINT_INTERVAL and remaining_time > 0:
                print(f"â±ï¸ Recording will end in: {remaining_time:.1f} seconds...")
                last_countdown_time = current_time
            
            # ì¹¨ë¬µ ì‹œê°„ì´ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ë…¹ìŒ ì¢…ë£Œ
            if elapsed_silence >= SILENCE_DURATION:
                recording = False
                recording_completed = True  # ë…¹ìŒ ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì • (ë” ì´ìƒ ì˜¤ë””ì˜¤ ì…ë ¥ì„ ë°›ì§€ ì•ŠìŒ)
                processing_audio = True
                print(f"\nâ¹ï¸ Recording ended automatically (silence for {SILENCE_DURATION} seconds).")
                processing_thread = threading.Thread(
                    target=process_recorded_audio_async,
                    daemon=True
                )
                processing_thread.start()
        else:
            # ì†Œë¦¬ê°€ ë‹¤ì‹œ ì„ê³„ê°’ ì´ìƒì´ ë˜ë©´ ì¹¨ë¬µ íƒ€ì´ë¨¸ ì´ˆê¸°í™”
            if silence_start_time is not None:
                print(f"ğŸ”Š Voice detected again ({current_db:.2f} dB > {SILENCE_THRESHOLD_DB} dB) - Silence timer reset")
                silence_start_time = None
def process_recorded_audio_async():
    """ë¹„ë™ê¸°ë¡œ ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
        # Play wait.mp3 before processing
    global processing_audio, recording_completed
    
    try:
        process_recorded_audio()
    finally:
        # ì²˜ë¦¬ ì™„ë£Œ í›„ í”Œë˜ê·¸ ë¦¬ì…‹
        processing_audio = False
        recording_completed = False
        print("\nğŸ¤ Ready for next recording...")
def process_recorded_audio():
    """ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
    global frames
    
    if not frames:  # Skip if no audio was recorded
        print("No audio recorded. Try again.")
        return
    
    print("\nğŸ”„ Processing recorded audio...")
    print("ğŸ”Š Playing wait message...")
    try:
        import pygame
        pygame.mixer.init()
        pygame.mixer.music.load(WAIT_AUDIO_FILE)
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy():
            pygame.time.Clock().tick(10)
    except ImportError:
        print(f"âš ï¸ pygame not installed. Cannot play {WAIT_AUDIO_FILE}")
    print("\nâ³ Waiting for VLM (Vision Language Model) result...")
    vlm_result = manager.get_vlm_result(timeout=40)
    if vlm_result:
        print(f"ğŸ¯ VLM analysis complete: {vlm_result[:100]}..." if len(vlm_result) > 100 else f"ğŸ¯ VLM analysis complete: {vlm_result}")
    else:
        print("âš ï¸ VLM processing timeout or failed")
    
    # Convert audio data to numpy array
    print("\nğŸ“Š Preparing audio data for transcription...")
    audio_data = np.concatenate(frames, axis=0)

    # 1. Convert audio to text using Whisper API
    print("\nğŸ™ï¸ Converting speech to text using Whisper API...")
    transcribed_text = convert_audio_to_text_via_api(audio_data, SAMPLE_RATE)

    if transcribed_text:
        print(f"âœ… Transcribed text: \"{transcribed_text}\"")

        # 2. Start conversation processing using the transcribed text (threaded)
        #    This will handle the conversational response and TTS.
        print("\nğŸ¤– Generating VIRUS conversational response (async thread)...")
        manager.start_text_conversation_processing(transcribed_text, vlm_result)

        # 3. Process command interpretation using the transcribed text (synchronous here)
        print("âš™ï¸ Interpreting robot commands...")
        command_response_json = None
        with api_lock: # Ensure API calls are thread-safe
            try:
                # Use process_for_commands (from LLM_function, expects text)
                command_response_json = process_for_commands( # ì—¬ê¸°ê°€ process_for_commands ì—¬ì•¼ í•©ë‹ˆë‹¤.
                    transcribed_text,
                    additional_prompt=f"[Image/Video Description]: {vlm_result}"
                )
                print(f"â†’ Robot command generated: {command_response_json}")
            except Exception as e:
                print(f"âŒ Command processing error: {e}")
                command_response_json = None
    else:
        print("âš ï¸ No transcribed text available for command processing")
        
    if command_response_json:
        print(f"\nğŸ¤– Executing robot command sequence...")
        try:
            run_spike(command_response_json)
            print("âœ… Robot command executed successfully")
        except Exception as e:
            print(f"âŒ Robot command execution error: {e}")
    else:
        print("â„¹ï¸ No robot command to execute")
    print("\nâ³ Waiting for conversational response to complete...")
    conversation_completed = manager.get_conversation_result(timeout = 40)
    if conversation_completed:
        print("âœ… Conversational response completed")
    else:
        print("âš ï¸ Conversational response timeout or failed")
        
    print("\nâœ… All processing completed. Ready for next command...")
    frames.clear()


def process_complete_interaction():
    """Main function to handle the complete interaction flow"""
    global processing_audio, recording_completed, recording
    processing_audio = False
    recording_completed = False
    recording = False
    # Create and start the audio stream
    stream = sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=CHANNELS,
        callback=audio_callback
    )
    stream.start()
    print("\nğŸš€ VIRUS System initialized successfully!")
    print("ğŸ¤ Voice detection active - speak to trigger recording")
    print(f"ğŸ“Š Recording triggers at >{THRESHOLD_DB} dB, stops after {SILENCE_DURATION}s of silence <{SILENCE_THRESHOLD_DB} dB")
    
    try:
        # í”„ë¡œê·¸ë¨ì´ ê³„ì† ì‹¤í–‰ë˜ë„ë¡ ë¬´í•œ ë£¨í”„ ìœ ì§€
        while True:
            time.sleep(0.1)  # ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€ìš© ì•½ê°„ì˜ ì§€ì—°
    
    except KeyboardInterrupt:
        print("\nğŸ”Œ Shutting down system.")
    finally:
        # Clean up resources

        if 'stream' in locals():
            stream.stop()
            stream.close()

if __name__ == "__main__":
    # Display welcome message
    print("=" * 50)
    print("VIRUS COMBAT ROBOT CONTROL SYSTEM")
    print("Versatile, Intelligent Robotic Unit for Strategy")
    print("=" * 50)
    print("Ready for voice commands - System features:")
    print(f"  â€¢ Voice threshold: {THRESHOLD_DB} dB (auto-start)")
    print(f"  â€¢ Silence detection: {SILENCE_DURATION}s below {SILENCE_THRESHOLD_DB} dB (auto-stop)")
    print(f"  â€¢ Audio format: {SAMPLE_RATE}Hz, 8-bit mono (optimized for Raspberry Pi)")
    print("  â€¢ Parallel processing: Vision analysis + Speech recognition + Response generation")
    print("  â€¢ Continuous operation: Automatically ready for next command after processing")
    print("\nPress Ctrl+C to exit anytime.")
    print("=" * 50)
    
    # Start the interaction loop
    process_complete_interaction() 
